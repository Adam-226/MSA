#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹è¯„ä¼°è„šæœ¬
ä¸“é—¨ç”¨äºè¯„ä¼°é€šè¿‡train_end_to_end.pyè®­ç»ƒçš„æ¨¡å‹
"""
import os
import sys
import json
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import argparse

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from src.models.end_to_end_model import EndToEndMultiModalModel
from src.preprocess.end_to_end_dataset import get_end_to_end_dataloaders
from src.training.config import load_config, get_device
from src.utils.evaluator import ModelEvaluator, compute_metrics, compute_metrics_per_class, plot_confusion_matrix

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æƒ…æ„Ÿæ ‡ç­¾
EMOTION_LABELS = ['neutral', 'joy', 'sadness', 'anger', 'surprise', 'fear', 'disgust']


class EndToEndModelEvaluator:
    """ç«¯åˆ°ç«¯æ¨¡å‹è¯„ä¼°å™¨ï¼Œä¸æ ‡å‡†ModelEvaluatoråŠŸèƒ½ä¸€è‡´"""
    
    def __init__(self, model: nn.Module, device: torch.device, emotion_labels: List[str] = None):
        """
        åˆå§‹åŒ–ç«¯åˆ°ç«¯æ¨¡å‹è¯„ä¼°å™¨
        
        Args:
            model: ç«¯åˆ°ç«¯æ¨¡å‹
            device: è®¾å¤‡
            emotion_labels: æƒ…æ„Ÿæ ‡ç­¾åˆ—è¡¨
        """
        self.model = model
        self.device = device
        self.emotion_labels = emotion_labels or EMOTION_LABELS
        
    def evaluate_on_loader(self, data_loader: DataLoader) -> Tuple[Dict, np.ndarray, np.ndarray]:
        """
        åœ¨æ•°æ®åŠ è½½å™¨ä¸Šè¯„ä¼°æ¨¡å‹
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            
        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡
            predictions: é¢„æµ‹ç»“æœ
            true_labels: çœŸå®æ ‡ç­¾
        """
        self.model.eval()
        
        all_preds = []
        all_labels = []
        all_logits = []
        total_loss = 0.0
        total_samples = 0
        
        with torch.no_grad():
            for batch in data_loader:
                # è·å–æ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                audio_features = batch['audio'].to(self.device)
                labels = batch['emotion'].to(self.device)
                
                batch_size = input_ids.size(0)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    audio_features=audio_features,
                    return_features=True
                )
                
                # è®¡ç®—æŸå¤±
                losses = self.model.calculate_losses(outputs, labels)
                loss = losses['total_loss']
                
                # ç´¯ç§¯æŸå¤±å’Œæ ·æœ¬æ•°
                total_loss += loss.item() * batch_size
                total_samples += batch_size
                
                # è·å–é¢„æµ‹ç»“æœ
                logits = outputs['logits']
                preds = torch.argmax(logits, dim=1)
                
                all_logits.extend(logits.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        predictions = np.array(all_preds)
        true_labels = np.array(all_labels)
        logits = np.array(all_logits)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'loss': total_loss / total_samples,
            'predictions': predictions.tolist(),
            'true_labels': true_labels.tolist(),
            'logits': logits.tolist()
        }
        
        # æ·»åŠ è¯¦ç»†æŒ‡æ ‡
        detailed_metrics = compute_metrics(predictions, true_labels)
        metrics.update(detailed_metrics)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, predictions)
        metrics['confusion_matrix'] = cm.tolist()
        
        return metrics, predictions, true_labels
    
    def generate_detailed_report(self, predictions: np.ndarray, true_labels: np.ndarray) -> Dict:
        """
        ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            true_labels: çœŸå®æ ‡ç­¾
            
        Returns:
            detailed_report: è¯¦ç»†æŠ¥å‘Š
        """
        # åŸºç¡€æŒ‡æ ‡
        basic_metrics = compute_metrics(predictions, true_labels)
        
        # æ¯ç±»åˆ«æŒ‡æ ‡
        per_class_metrics = compute_metrics_per_class(predictions, true_labels, self.emotion_labels)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(predictions, true_labels)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # sklearn classification report
        sklearn_report = classification_report(
            true_labels, predictions, 
            target_names=self.emotion_labels,
            output_dict=True,
            zero_division=0
        )
        
        # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
        unique_labels, label_counts = np.unique(true_labels, return_counts=True)
        label_distribution = {
            self.emotion_labels[label]: int(count) 
            for label, count in zip(unique_labels, label_counts)
        }
        
        # é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡
        unique_preds, pred_counts = np.unique(predictions, return_counts=True)
        pred_distribution = {
            self.emotion_labels[pred]: int(count) 
            for pred, count in zip(unique_preds, pred_counts)
        }
        
        detailed_report = {
            'basic_metrics': basic_metrics,
            'per_class_metrics': per_class_metrics,
            'sklearn_report': sklearn_report,
            'confusion_matrix': cm.tolist(),
            'confusion_matrix_normalized': cm_normalized.tolist(),
            'label_distribution': label_distribution,
            'pred_distribution': pred_distribution,
            'total_samples': len(true_labels)
        }
        
        return detailed_report
    
    def create_visualizations(self, predictions: np.ndarray, true_labels: np.ndarray, 
                            save_dir: str, history: Dict = None) -> Dict[str, str]:
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            true_labels: çœŸå®æ ‡ç­¾
            save_dir: ä¿å­˜ç›®å½•
            history: è®­ç»ƒå†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            saved_plots: ä¿å­˜çš„å›¾è¡¨è·¯å¾„
        """
        from src.utils.evaluator import (
            plot_confusion_matrix, plot_metrics_per_class, 
            plot_training_history, compute_confusion_matrix_func
        )
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        
        os.makedirs(save_dir, exist_ok=True)
        saved_plots = {}
        
        # 1. æ··æ·†çŸ©é˜µï¼ˆåŸå§‹ï¼‰
        cm = compute_confusion_matrix_func(predictions, true_labels)
        cm_raw_path = os.path.join(save_dir, 'confusion_matrix_raw.png')
        plot_confusion_matrix(
            cm=cm,
            classes=self.emotion_labels,
            normalize=False,
            title='Confusion Matrix (Raw Counts)',
            save_path=cm_raw_path
        )
        saved_plots['confusion_matrix_raw'] = cm_raw_path
        plt.close()
        
        # 2. æ··æ·†çŸ©é˜µï¼ˆå½’ä¸€åŒ–ï¼‰
        cm_norm_path = os.path.join(save_dir, 'confusion_matrix_normalized.png')
        plot_confusion_matrix(
            cm=cm,
            classes=self.emotion_labels,
            normalize=True,
            title='Confusion Matrix (Normalized)',
            save_path=cm_norm_path
        )
        saved_plots['confusion_matrix_normalized'] = cm_norm_path
        plt.close()
        
        # 3. æ¯ç±»åˆ«æŒ‡æ ‡
        per_class_metrics = compute_metrics_per_class(predictions, true_labels, self.emotion_labels)
        metrics_path = os.path.join(save_dir, 'metrics_per_class.png')
        plot_metrics_per_class(
            metrics_per_class=per_class_metrics,
            title='Performance Metrics by Emotion Class',
            save_path=metrics_path
        )
        saved_plots['metrics_per_class'] = metrics_path
        plt.close()
        
        # 4. ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
        self._plot_label_distribution(predictions, true_labels, save_dir)
        saved_plots['label_distribution'] = os.path.join(save_dir, 'label_distribution.png')
        
        # 5. è®­ç»ƒå†å²ï¼ˆå¦‚æœæä¾›ï¼‰
        if history is not None:
            history_path = os.path.join(save_dir, 'training_history.png')
            plot_training_history(history=history, save_path=history_path)
            saved_plots['training_history'] = history_path
            plt.close()
        
        # 6. è¯¦ç»†æ€§èƒ½çƒ­å›¾
        self._plot_detailed_heatmap(predictions, true_labels, save_dir)
        saved_plots['detailed_heatmap'] = os.path.join(save_dir, 'detailed_performance_heatmap.png')
        
        return saved_plots
    
    def _plot_label_distribution(self, predictions: np.ndarray, true_labels: np.ndarray, save_dir: str):
        """ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”å›¾"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        
        # ç»Ÿè®¡çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
        true_counts = np.bincount(true_labels, minlength=len(self.emotion_labels))
        pred_counts = np.bincount(predictions, minlength=len(self.emotion_labels))
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'Emotion': self.emotion_labels,
            'True Labels': true_counts,
            'Predictions': pred_counts
        })
        
        # è½¬æ¢ä¸ºé•¿æ ¼å¼
        df_long = pd.melt(df, id_vars=['Emotion'], var_name='Type', value_name='Count')
        
        # ç»˜å›¾
        plt.figure(figsize=(12, 6))
        sns.barplot(data=df_long, x='Emotion', y='Count', hue='Type')
        plt.title('True Labels vs Predictions Distribution')
        plt.xlabel('Emotion Category')
        plt.ylabel('Sample Count')
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        
        save_path = os.path.join(save_dir, 'label_distribution.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_detailed_heatmap(self, predictions: np.ndarray, true_labels: np.ndarray, save_dir: str):
        """ç»˜åˆ¶è¯¦ç»†æ€§èƒ½çƒ­å›¾"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        per_class_metrics = compute_metrics_per_class(predictions, true_labels, self.emotion_labels)
        
        # å‡†å¤‡æ•°æ®
        metrics_data = []
        for emotion in self.emotion_labels:
            if emotion in per_class_metrics:
                metrics_data.append([
                    per_class_metrics[emotion]['precision'],
                    per_class_metrics[emotion]['recall'],
                    per_class_metrics[emotion]['f1']
                ])
            else:
                metrics_data.append([0.0, 0.0, 0.0])
        
        metrics_array = np.array(metrics_data)
        
        # ç»˜åˆ¶çƒ­å›¾
        plt.figure(figsize=(8, 10))
        sns.heatmap(
            metrics_array,
            xticklabels=['Precision', 'Recall', 'F1-Score'],
            yticklabels=self.emotion_labels,
            annot=True,
            fmt='.3f',
            cmap='RdYlBu_r',
            cbar_kws={'label': 'Score'}
        )
        plt.title('Detailed Performance Heatmap by Emotion')
        plt.xlabel('Metrics')
        plt.ylabel('Emotion Category')
        plt.tight_layout()
        
        save_path = os.path.join(save_dir, 'detailed_performance_heatmap.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_results(self, results: Dict, save_path: str):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            results: è¯„ä¼°ç»“æœ
            save_path: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)
    
    def print_summary(self, results: Dict):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            results: è¯„ä¼°ç»“æœ
        """
        basic_metrics = results['basic_metrics']
        per_class = results['per_class_metrics']
        
        print("=" * 80)
        print("ç«¯åˆ°ç«¯æ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 80)
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {results['total_samples']}")
        print(f"ğŸ“ˆ å‡†ç¡®ç‡ (Accuracy): {basic_metrics['accuracy']:.4f}")
        print(f"ğŸ“ˆ F1-Macro: {basic_metrics['f1_macro']:.4f}")
        print(f"ğŸ“ˆ F1-Weighted: {basic_metrics['f1_weighted']:.4f}")
        print(f"ğŸ“ˆ Precision-Macro: {basic_metrics['precision_macro']:.4f}")
        print(f"ğŸ“ˆ Recall-Macro: {basic_metrics['recall_macro']:.4f}")
        
        print(f"\nå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
        print("-" * 80)
        print(f"{'ç±»åˆ«':<12} {'F1':<8} {'Precision':<10} {'Recall':<8} {'Support':<8}")
        print("-" * 80)
        
        for emotion in self.emotion_labels:
            if emotion in per_class:
                metrics = per_class[emotion]
                support = results['label_distribution'].get(emotion, 0)
                print(f"{emotion:<12} {metrics['f1']:<8.4f} {metrics['precision']:<10.4f} "
                      f"{metrics['recall']:<8.4f} {support:<8}")
        
        print("-" * 80)


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°ç«¯åˆ°ç«¯å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--split', type=str, default='test', choices=['test', 'dev'], help='è¯„ä¼°æ•°æ®é›†åˆ†å‰²')
    parser.add_argument('--output_dir', type=str, default='results', help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--save_predictions', action='store_true', help='æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    if not os.path.exists(args.checkpoint):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    device = get_device()
    
    logger.info(f"ğŸ”§ é…ç½®æ–‡ä»¶: {args.config}")
    logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹: {args.checkpoint}")
    logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    logger.info(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†: {args.split}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    dataloaders = get_end_to_end_dataloaders(config)
    
    if args.split == 'test':
        data_loader = dataloaders['test']
    else:
        data_loader = dataloaders['dev']
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ”¨ åˆ›å»ºç«¯åˆ°ç«¯æ¨¡å‹...")
    from src.models.end_to_end_model import create_end_to_end_model
    model = create_end_to_end_model(config)
    model = model.to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    logger.info("ğŸ“‚ åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹...")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®­ç»ƒepoch: {checkpoint.get('epoch', 'unknown')}")
    
    # è·å–å®éªŒç›®å½•
    checkpoint_dir = os.path.dirname(args.checkpoint)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EndToEndModelEvaluator(model, device, EMOTION_LABELS)
    
    # è¯„ä¼°æ¨¡å‹
    logger.info(f"ğŸ“Š åœ¨{args.split}é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    metrics, predictions, true_labels = evaluator.evaluate_on_loader(data_loader)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    logger.info("ğŸ“‹ ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š...")
    detailed_report = evaluator.generate_detailed_report(predictions, true_labels)
    
    # åŠ è½½è®­ç»ƒå†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    history = None
    history_path = os.path.join(checkpoint_dir, 'history.json')
    if os.path.exists(history_path):
        with open(history_path, 'r') as f:
            history = json.load(f)
    
    # åˆ›å»ºå¯è§†åŒ–
    logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    saved_plots = evaluator.create_visualizations(
        predictions, true_labels,
        save_dir=os.path.join(checkpoint_dir, 'evaluation_plots'),
        history=history
    )
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(checkpoint_dir, 'comprehensive_evaluation.json')
    final_results = {
        'evaluation_metrics': metrics,
        'detailed_report': detailed_report,
        'saved_plots': saved_plots,
        'model_info': {
            'checkpoint_path': args.checkpoint,
            'config_path': args.config,
            'model_config': config
        }
    }
    
    evaluator.save_results(final_results, results_path)
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary(detailed_report)
    
    logger.info(f"\nğŸ“„ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_path}")
    logger.info(f"ğŸ¯ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {os.path.join(checkpoint_dir, 'evaluation_plots')}")
    
    # å¯é€‰ï¼šä¿å­˜é¢„æµ‹ç»“æœ
    if args.save_predictions:
        predictions_file = os.path.join(args.output_dir, f"EndToEndEmotionModel_{args.split}_predictions.json")
        predictions_data = {
            'predictions': predictions.tolist(),
            'true_labels': true_labels.tolist(),
            'emotion_labels': EMOTION_LABELS
        }
        os.makedirs(args.output_dir, exist_ok=True)
        with open(predictions_file, 'w', encoding='utf-8') as f:
            json.dump(predictions_data, f, indent=4)
        logger.info(f"ğŸ“¦ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {predictions_file}")
    
    logger.info("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    
    return final_results


if __name__ == "__main__":
    main() 